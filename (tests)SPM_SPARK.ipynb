{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FALABELLA-GFSNEYDER/modelo-prediccion/blob/main/(tests)SPM_SPARK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_oA7HEc04Qv",
        "outputId": "10ce9343-e65d-4261-dbfe-45c213712ba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "import nltk\n",
        "import spacy\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "Zht3SuQqyBao",
        "outputId": "4be60291-a678-4294-a70e-8943bdfeddbd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  country sellerId        sellerName                             sku_seller  \\\n",
              "0      PE  SC206F5  CUBAEXPORTA PERU                             4564545154   \n",
              "1      PE  SC5949C    TODATECNOLOGIA             Aro con Espejo D2 / BLANCO   \n",
              "2      PE  SCD0365       COSMOBEAUTY  FCXL 2100_DELETED_2023-09-29_11-59-45   \n",
              "3      PE  SC5949C    TODATECNOLOGIA              ARO CON ESPEJO D2 / NEGRO   \n",
              "4      PE  SC35014          MONISSOS                          7891114026848   \n",
              "\n",
              "                                        product_name  brand_name  \\\n",
              "0         Hebermin Crema Cicatrizante Cubana 200 Gr.    GENERICO   \n",
              "1  Espejo Para Maquillaje Gato Con Luz Led con so...    GENERICO   \n",
              "2                                          Formax XL      FORMAX   \n",
              "3  Espejo Para Maquillaje Gato Con Luz Led con so...    GENERICO   \n",
              "4                 Caja Porta Herramientas Tramontina  TRAMONTINA   \n",
              "\n",
              "                     model   shop_sku                   primary_category  \\\n",
              "0                    Crema  123080732         Medicamentos farmaceuticos   \n",
              "1  Espejo de Maquillaje D2  121436410           Mobiliario de peluqueria   \n",
              "2                Formax XL       None           Mobiliario de peluqueria   \n",
              "3                       D2  121436975           Mobiliario de peluqueria   \n",
              "4                      Pro  119371765  Maletines|cajas para herramientas   \n",
              "\n",
              "  global_identifier  ... rlo_items_problema_cobro  \\\n",
              "0             G1606  ...                     <NA>   \n",
              "1             G1608  ...                     <NA>   \n",
              "2             G1608  ...                     <NA>   \n",
              "3             G1608  ...                     <NA>   \n",
              "4           G010105  ...                     <NA>   \n",
              "\n",
              "  rlo_items_producto_incompleto rlo_items_publicidad_enganosa  \\\n",
              "0                          <NA>                          <NA>   \n",
              "1                          <NA>                          <NA>   \n",
              "2                          <NA>                          <NA>   \n",
              "3                          <NA>                          <NA>   \n",
              "4                          <NA>                          <NA>   \n",
              "\n",
              "  rlo_items_falla_producto customer_reviews customer_reviews_1  \\\n",
              "0                     <NA>             <NA>               <NA>   \n",
              "1                     <NA>             <NA>               <NA>   \n",
              "2                     <NA>             <NA>               <NA>   \n",
              "3                     <NA>             <NA>               <NA>   \n",
              "4                     <NA>             <NA>               <NA>   \n",
              "\n",
              "  customer_reviews_2 customer_reviews_3 customer_reviews_4 customer_reviews_5  \n",
              "0               <NA>               <NA>               <NA>               <NA>  \n",
              "1               <NA>               <NA>               <NA>               <NA>  \n",
              "2               <NA>               <NA>               <NA>               <NA>  \n",
              "3               <NA>               <NA>               <NA>               <NA>  \n",
              "4               <NA>               <NA>               <NA>               <NA>  \n",
              "\n",
              "[5 rows x 176 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9457e881-1daa-46c1-81c0-0f815cc65e46\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country</th>\n",
              "      <th>sellerId</th>\n",
              "      <th>sellerName</th>\n",
              "      <th>sku_seller</th>\n",
              "      <th>product_name</th>\n",
              "      <th>brand_name</th>\n",
              "      <th>model</th>\n",
              "      <th>shop_sku</th>\n",
              "      <th>primary_category</th>\n",
              "      <th>global_identifier</th>\n",
              "      <th>...</th>\n",
              "      <th>rlo_items_problema_cobro</th>\n",
              "      <th>rlo_items_producto_incompleto</th>\n",
              "      <th>rlo_items_publicidad_enganosa</th>\n",
              "      <th>rlo_items_falla_producto</th>\n",
              "      <th>customer_reviews</th>\n",
              "      <th>customer_reviews_1</th>\n",
              "      <th>customer_reviews_2</th>\n",
              "      <th>customer_reviews_3</th>\n",
              "      <th>customer_reviews_4</th>\n",
              "      <th>customer_reviews_5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PE</td>\n",
              "      <td>SC206F5</td>\n",
              "      <td>CUBAEXPORTA PERU</td>\n",
              "      <td>4564545154</td>\n",
              "      <td>Hebermin Crema Cicatrizante Cubana 200 Gr.</td>\n",
              "      <td>GENERICO</td>\n",
              "      <td>Crema</td>\n",
              "      <td>123080732</td>\n",
              "      <td>Medicamentos farmaceuticos</td>\n",
              "      <td>G1606</td>\n",
              "      <td>...</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PE</td>\n",
              "      <td>SC5949C</td>\n",
              "      <td>TODATECNOLOGIA</td>\n",
              "      <td>Aro con Espejo D2 / BLANCO</td>\n",
              "      <td>Espejo Para Maquillaje Gato Con Luz Led con so...</td>\n",
              "      <td>GENERICO</td>\n",
              "      <td>Espejo de Maquillaje D2</td>\n",
              "      <td>121436410</td>\n",
              "      <td>Mobiliario de peluqueria</td>\n",
              "      <td>G1608</td>\n",
              "      <td>...</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PE</td>\n",
              "      <td>SCD0365</td>\n",
              "      <td>COSMOBEAUTY</td>\n",
              "      <td>FCXL 2100_DELETED_2023-09-29_11-59-45</td>\n",
              "      <td>Formax XL</td>\n",
              "      <td>FORMAX</td>\n",
              "      <td>Formax XL</td>\n",
              "      <td>None</td>\n",
              "      <td>Mobiliario de peluqueria</td>\n",
              "      <td>G1608</td>\n",
              "      <td>...</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PE</td>\n",
              "      <td>SC5949C</td>\n",
              "      <td>TODATECNOLOGIA</td>\n",
              "      <td>ARO CON ESPEJO D2 / NEGRO</td>\n",
              "      <td>Espejo Para Maquillaje Gato Con Luz Led con so...</td>\n",
              "      <td>GENERICO</td>\n",
              "      <td>D2</td>\n",
              "      <td>121436975</td>\n",
              "      <td>Mobiliario de peluqueria</td>\n",
              "      <td>G1608</td>\n",
              "      <td>...</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PE</td>\n",
              "      <td>SC35014</td>\n",
              "      <td>MONISSOS</td>\n",
              "      <td>7891114026848</td>\n",
              "      <td>Caja Porta Herramientas Tramontina</td>\n",
              "      <td>TRAMONTINA</td>\n",
              "      <td>Pro</td>\n",
              "      <td>119371765</td>\n",
              "      <td>Maletines|cajas para herramientas</td>\n",
              "      <td>G010105</td>\n",
              "      <td>...</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 176 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9457e881-1daa-46c1-81c0-0f815cc65e46')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9457e881-1daa-46c1-81c0-0f815cc65e46 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9457e881-1daa-46c1-81c0-0f815cc65e46');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0dc41b43-4bcf-40c1-ae7e-edf0aea71887\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0dc41b43-4bcf-40c1-ae7e-edf0aea71887')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0dc41b43-4bcf-40c1-ae7e-edf0aea71887 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "# Autenticación\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Proyecto y cliente de BigQuery\n",
        "project_id = 'bi-fcom-drmb-local-pe-sbx'\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Consulta\n",
        "query = f\"\"\"\n",
        "    SELECT *\n",
        "    FROM `bi-fcom-drmb-local-pe-sbx.Dragonite_SX_KPIs.reporte_productos`\n",
        "    WHERE country = 'PE'\n",
        "    LIMIT 15000\n",
        "\"\"\"\n",
        "\n",
        "# Cargar el DataFrame\n",
        "df_spm = client.query(query).to_dataframe()\n",
        "\n",
        "# Ver las primeras filas del DataFrame\n",
        "df_spm.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WReBq7Pa3BIn"
      },
      "outputs": [],
      "source": [
        "#print(df_spm.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eLkcZJlF3F0R"
      },
      "outputs": [],
      "source": [
        "# Cargar el conjunto de datos desde el DataFrame de BigQuery\n",
        "df = df_spm\n",
        "\n",
        "# Lista de atributos que deseas mantener\n",
        "atributos_deseados = [\n",
        "    'country', 'sellerId', 'sellerName', 'sku_seller', 'product_name','shop_sku',\n",
        "    'primary_category','N1', 'size', 'width_in_cm','length_in_cm', 'height_in_cm',\n",
        "    'weight_in_kg', 'width_diff', 'length_diff','height_diff', 'weight_diff'\n",
        "]\n",
        "\n",
        "# Crear un nuevo DataFrame solo con los atributos deseados y renombrarlo a df_spm\n",
        "df_spm = df[atributos_deseados]\n",
        "\n",
        "# Mostrar las primeras filas del nuevo DataFrame\n",
        "#df_spm.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VtH1RcrL3TKj"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Ver cantidad de nulos\n",
        "#df_spm.isnull().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dmsFh5nK3X7p"
      },
      "outputs": [],
      "source": [
        "# Eliminar todas las filas que tengan nulos\n",
        "df_spm = df_spm.dropna()\n",
        "\n",
        "#Ver la cantidad de nulos, post imputación\n",
        "#df_spm.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gW1QYOAc3kXO"
      },
      "outputs": [],
      "source": [
        "# Suma de los valores \"No\" y \"Sí\" para cada columna\n",
        "suma_no_si_width_diff = df_spm['width_diff'].value_counts().to_dict()\n",
        "suma_no_si_length_diff = df_spm['length_diff'].value_counts().to_dict()\n",
        "suma_no_si_height_diff = df_spm['height_diff'].value_counts().to_dict()\n",
        "suma_no_si_weight_diff = df_spm['weight_diff'].value_counts().to_dict()\n",
        "\n",
        "# Mostrar los resultados\n",
        "#print(\"Suma de 'No' y 'Sí' en width_diff:\", suma_no_si_width_diff)\n",
        "#print(\"Suma de 'No' y 'Sí' en length_diff:\", suma_no_si_length_diff)\n",
        "#print(\"Suma de 'No' y 'Sí' en height_diff:\", suma_no_si_height_diff)\n",
        "#print(\"Suma de 'No' y 'Sí' en weight_diff:\", suma_no_si_weight_diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "S976O0QI3p9B"
      },
      "outputs": [],
      "source": [
        "df_spm['target'] = 'Correcto'  # Inicializar con 'Correcto'\n",
        "\n",
        "# Actualizar a 'Incorrecto' si hay algún 'Sí' en las columnas mencionadas\n",
        "condicion_si = (df_spm['width_diff'] == 'Si') | \\\n",
        "               (df_spm['length_diff'] == 'Si') | \\\n",
        "               (df_spm['height_diff'] == 'Si') | \\\n",
        "               (df_spm['weight_diff'] == 'Si')\n",
        "\n",
        "df_spm.loc[condicion_si, 'target'] = 'Incorrecto'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FBgZ04og33L8"
      },
      "outputs": [],
      "source": [
        "# Eliminar Incorrectos\n",
        "df_spm_correctos = df_spm[df_spm['target'] == 'Correcto']\n",
        "\n",
        "# Renombrar el DataFrame\n",
        "df_spm = df_spm_correctos\n",
        "\n",
        "# Ver las primeras filas del DataFrame resultante\n",
        "#df_spm.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9GUyUvyX4A-t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4296d4bc-f0de-444d-e9c1-d79723370a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correcto    12817\n",
            "Name: target, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Suma de los valores en la columna 'target'\n",
        "suma_target = df_spm['target'].value_counts()\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(suma_target)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fjb1uwri4I16"
      },
      "outputs": [],
      "source": [
        "# Lista de atributos que deseas mantener\n",
        "atributos_deseados = [\n",
        "    'sellerName','product_name','shop_sku','primary_category','N1', 'size',\n",
        "    'width_in_cm','length_in_cm', 'height_in_cm','weight_in_kg',\n",
        "]\n",
        "\n",
        "# Crear un nuevo DataFrame solo con los atributos deseados y renombrarlo a df_spm\n",
        "df_spm = df[atributos_deseados]\n",
        "\n",
        "# Mostrar las primeras filas del nuevo DataFrame\n",
        "#df_spm.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Cbijv76C4gUr"
      },
      "outputs": [],
      "source": [
        "df_spm = df_spm.dropna()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vbedQTG64TQX"
      },
      "outputs": [],
      "source": [
        "#Ver la cantidad de nulos, post imputación\n",
        "#df_spm.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "l0Zk_Dyk4jux"
      },
      "outputs": [],
      "source": [
        "# Convertir las columnas a tipo numérico\n",
        "df_spm['width_in_cm'] = pd.to_numeric(df_spm['width_in_cm'], errors='coerce')\n",
        "df_spm['length_in_cm'] = pd.to_numeric(df_spm['length_in_cm'], errors='coerce')\n",
        "df_spm['height_in_cm'] = pd.to_numeric(df_spm['height_in_cm'], errors='coerce')\n",
        "\n",
        "# Calcular la columna de volumen en cm3\n",
        "df_spm['volumen_cm3'] = df_spm['width_in_cm'] * df_spm['length_in_cm'] * df_spm['height_in_cm']\n",
        "\n",
        "# Reordenar las columnas colocando 'size' al final\n",
        "column_order = [col for col in df_spm.columns if col != 'size'] + ['size']\n",
        "df_spm = df_spm[column_order]\n",
        "\n",
        "# Verificar las primeras filas del DataFrame actualizado\n",
        "#df_spm.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, CountVectorizer, IDF\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n"
      ],
      "metadata": {
        "id": "t7gWVwejFgTl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir las columnas a tipo numérico con 4 decimales\n",
        "numeric_columns = ['width_in_cm', 'length_in_cm', 'height_in_cm', 'weight_in_kg', 'volumen_cm3']\n",
        "for col in numeric_columns:\n",
        "    df_spm[col] = pd.to_numeric(df_spm[col], errors='coerce').round(4)\n",
        "\n",
        "# Reordenar las columnas colocando 'size' al final\n",
        "column_order = [col for col in df_spm.columns if col != 'size'] + ['size']\n",
        "df_spm = df_spm[column_order]\n",
        "\n",
        "# Verificar las primeras filas del DataFrame actualizado\n",
        "#df_spm.head()"
      ],
      "metadata": {
        "id": "BYaIxsxhFntp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sSTKbxHT8ATY"
      },
      "outputs": [],
      "source": [
        "# Crear un esquema para el DataFrame de Spark\n",
        "schema = StructType([\n",
        "    StructField(\"sellerName\", StringType(), True),\n",
        "    StructField(\"product_name\", StringType(), True),\n",
        "    StructField(\"shop_sku\", StringType(), True),\n",
        "    StructField(\"primary_category\", StringType(), True),\n",
        "    StructField(\"N1\", StringType(), True),\n",
        "    StructField(\"width_in_cm\", FloatType(), True),\n",
        "    StructField(\"length_in_cm\", FloatType(), True),\n",
        "    StructField(\"height_in_cm\", FloatType(), True),\n",
        "    StructField(\"weight_in_kg\", FloatType(), True),\n",
        "    StructField(\"volumen_cm3\", FloatType(), True),\n",
        "    StructField(\"size\", StringType(), True),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar la sesión de Spark\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"SPM-SPARK\").getOrCreate()\n",
        "\n",
        "# Crear el DataFrame de Spark a partir del DataFrame de pandas\n",
        "df_spark = spark.createDataFrame(df_spm, schema=schema)\n",
        "\n",
        "# Verificar las primeras filas del DataFrame de Spark\n",
        "#df_spark.show()"
      ],
      "metadata": {
        "id": "PpTtwjrCFta4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tzwom-dr-oFD",
        "outputId": "49e74ce7-ec73-4390-b5d0-3c7f749b91e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "2024-01-22 17:52:03.808641: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-22 17:52:03.808735: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-22 17:52:03.810621: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-22 17:52:05.741712: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting es-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.6.0/es_core_news_sm-3.6.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "from pyspark.ml import Pipeline\n",
        "import pyspark.sql.functions as F\n",
        "import nltk\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wemHq_XrGXSm",
        "outputId": "831b9851-e4bc-4c43-e33a-84adcf29ebfe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar modelo de procesamiento de lenguaje natural en español\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "# Obtener las stopwords en español\n",
        "spanish_stop_words = set(nltk.corpus.stopwords.words('spanish'))\n",
        "\n",
        "# Definir la función UDF para lematizar texto\n",
        "def lemmatize_text_udf(text):\n",
        "    doc = nlp(text)\n",
        "    return ' '.join([token.lemma_ for token in doc])\n",
        "\n",
        "# Definir las funciones UDF para tokenizar y limpiar texto\n",
        "def tokenize_and_clean_udf(text):\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in spanish_stop_words]\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "bYwJaYkkGcAT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir las funciones UDF para Spark\n",
        "tokenize_and_clean_spark_udf = udf(tokenize_and_clean_udf, ArrayType(StringType()))\n",
        "lemmatize_text_spark_udf = udf(lemmatize_text_udf, StringType())"
      ],
      "metadata": {
        "id": "K9eAkMrPHjGc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar tokenización y limpieza en un solo paso\n",
        "df_spark = df_spark.withColumn('product_name_tokens', tokenize_and_clean_spark_udf('product_name'))\n",
        "df_spark = df_spark.withColumn('primary_category_tokens', tokenize_and_clean_spark_udf('primary_category'))\n",
        "df_spark = df_spark.withColumn('product_name_cleaned', lemmatize_text_spark_udf(F.concat_ws(' ', 'product_name_tokens')))\n",
        "df_spark = df_spark.withColumn('primary_category_cleaned', lemmatize_text_spark_udf(F.concat_ws(' ', 'primary_category_tokens')))"
      ],
      "metadata": {
        "id": "bil7XUNSIBv3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar lematización\n",
        "df_spark = df_spark.withColumn('product_name_cleaned', lemmatize_text_spark_udf('product_name_cleaned'))\n",
        "df_spark = df_spark.withColumn('primary_category_cleaned', lemmatize_text_spark_udf('primary_category_cleaned'))\n"
      ],
      "metadata": {
        "id": "elb0sjl0IBg_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF para 'product_name' y 'primary_category' en una única pipeline\n",
        "vectorizer_name = CountVectorizer(inputCol='product_name_tokens', outputCol='raw_name_counts_name_tfidf')\n",
        "idf_name = IDF(inputCol='raw_name_counts_name_tfidf', outputCol='name_tfidf')\n",
        "\n",
        "vectorizer_category = CountVectorizer(inputCol='primary_category_tokens', outputCol='raw_category_counts_category_tfidf')\n",
        "idf_category = IDF(inputCol='raw_category_counts_category_tfidf', outputCol='category_tfidf')\n"
      ],
      "metadata": {
        "id": "83KOmBfCIBeU"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_pipeline = Pipeline(stages=[\n",
        "    vectorizer_name, idf_name,\n",
        "    vectorizer_category, idf_category\n",
        "])\n",
        "\n",
        "# Verificar las primeras filas del DataFrame de Spark después del procesamiento de texto\n",
        "#df_spark.show()"
      ],
      "metadata": {
        "id": "eGjuR-ECIBbT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0et22OdGCeTu"
      },
      "outputs": [],
      "source": [
        "#df_spark.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1-ffj4mVFpTS"
      },
      "outputs": [],
      "source": [
        "#rdd_from_df_spark = df_spark.rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "kSSvx0S3DGrr"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Aplicar el pipeline de procesamiento de texto al DataFrame\n",
        "df_spark = text_pipeline.fit(df_spark).transform(df_spark)\n",
        "\n",
        "# Reducción de palabras a las dos primeras en las columnas 'product_name_cleaned' y 'primary_category_cleaned'\n",
        "df_spark = df_spark.withColumn('product_name_cleaned', F.expr(\"substring_index(product_name_cleaned, ' ', 2)\"))\n",
        "df_spark = df_spark.withColumn('primary_category_cleaned', F.expr(\"substring_index(primary_category_cleaned, ' ', 2)\"))\n",
        "\n",
        "# Verificar las primeras filas del DataFrame de Spark después del procesamiento de texto\n",
        "#df_spark.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un StringIndexer para convertir 'size' a etiquetas numéricas\n",
        "indexer = StringIndexer(inputCol='size', outputCol='size_indexed')\n",
        "\n",
        "# Ajustar las columnas de características según tu conjunto de datos, incluyendo las nuevas características de TF-IDF\n",
        "feature_columns = ['width_in_cm', 'length_in_cm', 'height_in_cm', 'weight_in_kg', 'volumen_cm3']\n",
        "\n",
        "# Crear un VectorAssembler para combinar las características en un vector\n",
        "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
        "\n",
        "# Dividir el conjunto de datos en entrenamiento y prueba\n",
        "train_data, test_data = df_spark.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Modelo de clasificación\n",
        "classifier = DecisionTreeClassifier(\n",
        "    labelCol='size_indexed',  # Usar la columna size_indexed como etiqueta\n",
        "    featuresCol='features',\n",
        "    maxDepth=30,\n",
        "    minInstancesPerNode=1\n",
        ")\n",
        "\n",
        "# Crear el Pipeline con todas las etapas\n",
        "pipeline = Pipeline(stages=[\n",
        "    indexer,\n",
        "    vector_assembler,\n",
        "    classifier\n",
        "])\n"
      ],
      "metadata": {
        "id": "PcKLwYg7Y1Vp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajustar el modelo al conjunto de entrenamiento\n",
        "model = pipeline.fit(train_data)"
      ],
      "metadata": {
        "id": "x2B6fxZfCT6U"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones sobre los datos de prueba.\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Visualizar predicciones\n",
        "predictions.select('size', 'size_indexed', 'prediction', 'probability').show()\n",
        "\n",
        "# Crear un evaluador\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='size_indexed', predictionCol='prediction', metricName='accuracy')\n",
        "\n",
        "# Calcular el accuracy\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f'Exactitud del modelo: {accuracy}')\n"
      ],
      "metadata": {
        "id": "BANWSnkYY5hw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a32c58e4-b987-454f-e87d-a36a0b25f1e6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------+----------+--------------------+\n",
            "|size|size_indexed|prediction|         probability|\n",
            "+----+------------+----------+--------------------+\n",
            "| XS3|         2.0|       2.0|[0.0,0.0,1.0,0.0,...|\n",
            "|   M|         3.0|       3.0|[0.0,0.0,0.0,1.0,...|\n",
            "| XS2|         0.0|       0.0|[1.0,0.0,0.0,0.0,...|\n",
            "| XS3|         2.0|       2.0|[0.0,0.0,1.0,0.0,...|\n",
            "|   M|         3.0|       3.0|[0.0,0.0,0.0,1.0,...|\n",
            "|  XS|         1.0|       1.0|[0.0,1.0,0.0,0.0,...|\n",
            "| XS2|         0.0|       0.0|[1.0,0.0,0.0,0.0,...|\n",
            "| XS2|         0.0|       0.0|[1.0,0.0,0.0,0.0,...|\n",
            "| XS3|         2.0|       2.0|[0.0,0.0,1.0,0.0,...|\n",
            "| XS3|         2.0|       2.0|[0.0,0.0,1.0,0.0,...|\n",
            "| XS3|         2.0|       2.0|[0.0,0.0,1.0,0.0,...|\n",
            "| XS3|         2.0|       2.0|[0.0,0.0,1.0,0.0,...|\n",
            "| XS3|         2.0|       2.0|[0.0,0.0,1.0,0.0,...|\n",
            "|   M|         3.0|       3.0|[0.0,0.0,0.0,1.0,...|\n",
            "|   M|         3.0|       3.0|[0.0,0.0,0.0,1.0,...|\n",
            "| XS2|         0.0|       0.0|[1.0,0.0,0.0,0.0,...|\n",
            "| XS2|         0.0|       0.0|[1.0,0.0,0.0,0.0,...|\n",
            "| XS3|         2.0|       2.0|[0.0,0.0,1.0,0.0,...|\n",
            "| XS2|         0.0|       0.0|[1.0,0.0,0.0,0.0,...|\n",
            "| XS2|         0.0|       0.0|[1.0,0.0,0.0,0.0,...|\n",
            "+----+------------+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Exactitud del modelo: 0.9707190511489993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "HO09-Yy7CirP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf5fc622-d3a5-4a47-a0cd-e410245e6b9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------+----------+\n",
            "|size|size_indexed|prediction|\n",
            "+----+------------+----------+\n",
            "|   S|         4.0|       4.0|\n",
            "| XS2|         0.0|       3.0|\n",
            "|   L|         5.0|       3.0|\n",
            "|  LO|         6.0|       3.0|\n",
            "| XS2|         0.0|       1.0|\n",
            "| XXL|        10.0|       9.0|\n",
            "|   O|         9.0|       5.0|\n",
            "|   M|         3.0|       7.0|\n",
            "|   M|         3.0|       0.0|\n",
            "|   O|         9.0|       7.0|\n",
            "| XXL|        10.0|       5.0|\n",
            "|  XS|         1.0|       3.0|\n",
            "|   S|         4.0|       1.0|\n",
            "|  LO|         6.0|       6.0|\n",
            "|   L|         5.0|       5.0|\n",
            "|  XL|         7.0|       3.0|\n",
            "|  EO|         8.0|       3.0|\n",
            "|  XL|         7.0|      10.0|\n",
            "|  XS|         1.0|       1.0|\n",
            "| XS3|         2.0|       2.0|\n",
            "+----+------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Mostrar los valores distintos de size, label y prediction\n",
        "predictions.select('size', 'size_indexed', 'prediction').distinct().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "IJaP_wrILPpA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64ce109e-f753-43d0-d091-81aefb56179c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall del modelo: 0.9707190511489993\n"
          ]
        }
      ],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(labelCol='size_indexed', predictionCol='prediction', metricName='weightedRecall')\n",
        "recall = evaluator.evaluate(predictions)\n",
        "print(f'Recall del modelo: {recall}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "_DPa2e_nLqWY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d861bf65-595a-4733-abb9-03d0d639ac04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión del modelo: 0.9705917006044475\n"
          ]
        }
      ],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(labelCol='size_indexed', predictionCol='prediction', metricName='weightedPrecision')\n",
        "precision = evaluator.evaluate(predictions)\n",
        "print(f'Precisión del modelo: {precision}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgmFlSy-LtyV"
      },
      "outputs": [],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(labelCol='size_indexed', predictionCol='prediction', metricName='f1')\n",
        "f1_score = evaluator.evaluate(predictions)\n",
        "print(f'F1-Score del modelo: {f1_score}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AVp29Eu_mM7"
      },
      "outputs": [],
      "source": [
        "# Ruta local donde se guardará el modelo\n",
        "local_model_path = \"/content/SPM_spark\"\n",
        "\n",
        "# Guardar el modelo en el sistema de archivos local de Colab\n",
        "model.save(local_model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KSTUGkY_xHe"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Comprimir el modelo para descargarlo como un archivo zip\n",
        "!zip -r /content/SPM_spark.zip /content/SPM_spark\n",
        "\n",
        "# Descargar el archivo zip\n",
        "files.download(\"/content/SPM_spark.zip\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_boroU68CSmj"
      },
      "source": [
        "# --- FINAL DEL MODELO TALLAS ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbo2oYhDCYyJ"
      },
      "source": [
        "# CARGAR EL MODELO Y PROBAR CON UN REGISTRO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrUHsEHKEdtT",
        "outputId": "7b76e9db-0db0-4160-a9e6-d21a79679016"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=eb28f6cca8bc74fb5e6f2c8bedb01900a1843f438aeb0854d5119328da1696dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFhDcNk_2YUq"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from pyspark.ml import PipelineModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI32Pv8e5ho8",
        "outputId": "50427ac0-1e5b-47a7-973a-ea2ef1decfc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contenido del directorio:\n",
            "stages\n",
            "metadata\n",
            "\n",
            "¡La carpeta 'stages' existe! Puede contener el modelo.\n",
            "\n",
            "¡La carpeta 'metadata' existe! Puede contener información adicional sobre el modelo.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "model_directory = \"/content/SPM_spark\"\n",
        "\n",
        "# Verificar si el directorio existe\n",
        "if os.path.exists(model_directory):\n",
        "    # Listar el contenido del directorio\n",
        "    content_list = os.listdir(model_directory)\n",
        "\n",
        "    # Imprimir el contenido del directorio\n",
        "    print(\"Contenido del directorio:\")\n",
        "    for item in content_list:\n",
        "        print(item)\n",
        "\n",
        "    # Verificar si hay una carpeta 'stages'\n",
        "    if 'stages' in content_list:\n",
        "        print(\"\\n¡La carpeta 'stages' existe! Puede contener el modelo.\")\n",
        "\n",
        "    # Verificar si hay una carpeta 'metadata'\n",
        "    if 'metadata' in content_list:\n",
        "        print(\"\\n¡La carpeta 'metadata' existe! Puede contener información adicional sobre el modelo.\")\n",
        "else:\n",
        "    print(f\"El directorio {model_directory} no existe.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h59ZoFmN4ZCF"
      },
      "outputs": [],
      "source": [
        "#Extraerel zip\n",
        "# Nombre del archivo zip que subiste\n",
        "zip_filename = 'SPM_spark.zip'\n",
        "\n",
        "# Descomprimir el archivo\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall('SPM_spark')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVuljDZSCd2r"
      },
      "outputs": [],
      "source": [
        "#Cargar el modelo\n",
        "# Cargar el modelo desde el directorio descomprimido\n",
        "loaded_model = PipelineModel.load(\"/content/SPM_spark\")\n",
        "\n",
        "# Ahora puedes utilizar el modelo cargado en Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8vwj9fTDkfG"
      },
      "outputs": [],
      "source": [
        "nuevo_registro = {\n",
        "    'sellerName': 'FARMATODO Shop',\n",
        "    'product_name': 'Dolex activgel 24 tabs',\n",
        "    'shop_sku': '119878204',\n",
        "    'primary_category': 'Medicamentos farmaceuticos',\n",
        "    'N1': 'Belleza, cuidado personal, higiene y salud',\n",
        "    'width_in_cm': 10.0,\n",
        "    'length_in_cm': 7.0,\n",
        "    'height_in_cm': 10.0,\n",
        "    'weight_in_kg': 0.3,\n",
        "    'volumen_cm3': 700.0,\n",
        "    'size': 'XS3'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQc_vGr-OwaD"
      },
      "outputs": [],
      "source": [
        "# Importar pandas\n",
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Crear un esquema para el DataFrame de Spark\n",
        "schema = StructType([\n",
        "    StructField(\"sellerName\", StringType(), True),\n",
        "    StructField(\"product_name\", StringType(), True),\n",
        "    StructField(\"shop_sku\", StringType(), True),\n",
        "    StructField(\"primary_category\", StringType(), True),\n",
        "    StructField(\"N1\", StringType(), True),\n",
        "    StructField(\"width_in_cm\", FloatType(), True),\n",
        "    StructField(\"length_in_cm\", FloatType(), True),\n",
        "    StructField(\"height_in_cm\", FloatType(), True),\n",
        "    StructField(\"weight_in_kg\", FloatType(), True),\n",
        "    StructField(\"volumen_cm3\", FloatType(), True),\n",
        "    StructField(\"size\", StringType(), True),\n",
        "])\n",
        "\n",
        "# Inicializar la sesión de Spark\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"SPM-SPARK\").getOrCreate()\n",
        "\n",
        "\n",
        "# Definir la función UDF para lematizar texto\n",
        "def lemmatize_text_udf(text):\n",
        "    doc = nlp(text)\n",
        "    return ' '.join([token.lemma_ for token in doc])\n",
        "\n",
        "# Definir las funciones UDF para tokenizar y limpiar texto\n",
        "def tokenize_and_clean_udf(text):\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in spanish_stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Definir las funciones UDF para Spark\n",
        "tokenize_and_clean_spark_udf = udf(tokenize_and_clean_udf, ArrayType(StringType()))\n",
        "lemmatize_text_spark_udf = udf(lemmatize_text_udf, StringType())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXhOsTPSKtux",
        "outputId": "6a055fdf-b426-4779-f9c4-25279ed29ab4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----+------------+----------+--------------------+\n",
            "|        product_name|size|size_indexed|prediction|         probability|\n",
            "+--------------------+----+------------+----------+--------------------+\n",
            "|Dolex activgel 24...| XS3|         1.0|       1.0|[0.0,1.0,0.0,0.0,...|\n",
            "+--------------------+----+------------+----------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Crear un DataFrame de pandas con el nuevo registro\n",
        "nuevo_registro_pd = pd.DataFrame([nuevo_registro])\n",
        "\n",
        "# Crear un DataFrame de Spark a partir del DataFrame de pandas\n",
        "nuevo_registro_spark = spark.createDataFrame(nuevo_registro_pd, schema=schema)\n",
        "\n",
        "# Aplicar tokenización y limpieza en un solo paso al nuevo registro\n",
        "nuevo_registro_spark = nuevo_registro_spark.withColumn('product_name_tokens', tokenize_and_clean_spark_udf('product_name'))\n",
        "nuevo_registro_spark = nuevo_registro_spark.withColumn('primary_category_tokens', tokenize_and_clean_spark_udf('primary_category'))\n",
        "nuevo_registro_spark = nuevo_registro_spark.withColumn('product_name_cleaned', lemmatize_text_spark_udf(F.concat_ws(' ', 'product_name_tokens')))\n",
        "nuevo_registro_spark = nuevo_registro_spark.withColumn('primary_category_cleaned', lemmatize_text_spark_udf(F.concat_ws(' ', 'primary_category_tokens')))\n",
        "nuevo_registro_spark = nuevo_registro_spark.withColumn('product_name_cleaned', lemmatize_text_spark_udf('product_name_cleaned'))\n",
        "nuevo_registro_spark = nuevo_registro_spark.withColumn('primary_category_cleaned', lemmatize_text_spark_udf('primary_category_cleaned'))\n",
        "\n",
        "# Aplicar el mismo pipeline del modelo al nuevo registro\n",
        "resultado_prediccion = loaded_model.transform(nuevo_registro_spark)\n",
        "\n",
        "# Seleccionar las columnas relevantes del resultado de la predicción\n",
        "resultado_seleccionado = resultado_prediccion.select('product_name', 'size', 'size_indexed', 'prediction', 'probability').show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxcUgPVCJTfo"
      },
      "source": [
        "#TERMINA TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kR06pnLJYEe"
      },
      "source": [
        "# FIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z0JN91CNeV8",
        "outputId": "98bf3c83-0de3-4b52-fa78-9ca76f4c9ffa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+--------------+--------+--------------------+------------+-----------+------------+------------+------------+-----------+----+\n",
            "|  sellerName|  product_name|shop_sku|    primary_category|          N1|width_in_cm|length_in_cm|height_in_cm|weight_in_kg|volumen_cm3|size|\n",
            "+------------+--------------+--------+--------------------+------------+-----------+------------+------------+------------+-----------+----+\n",
            "|Sneyder Shop|Refrigueradora|  SKU183|Electronomesticos...|Linea Blanca|     1000.0|      2400.0|       100.0|       300.0|      2.4E8|  EO|\n",
            "+------------+--------------+--------+--------------------+------------+-----------+------------+------------+------------+-----------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
        "\n",
        "\n",
        "# Crear un esquema para el DataFrame de Spark\n",
        "schema = StructType([\n",
        "    StructField(\"sellerName\", StringType(), True),\n",
        "    StructField(\"product_name\", StringType(), True),\n",
        "    StructField(\"shop_sku\", StringType(), True),\n",
        "    StructField(\"primary_category\", StringType(), True),\n",
        "    StructField(\"N1\", StringType(), True),\n",
        "    StructField(\"width_in_cm\", FloatType(), True),\n",
        "    StructField(\"length_in_cm\", FloatType(), True),\n",
        "    StructField(\"height_in_cm\", FloatType(), True),\n",
        "    StructField(\"weight_in_kg\", FloatType(), True),\n",
        "    StructField(\"volumen_cm3\", FloatType(), True),\n",
        "    StructField(\"size\", StringType(), True),\n",
        "])\n",
        "\n",
        "# Inicializar la sesión de Spark\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"SPM-SPARK\").getOrCreate()\n",
        "\n",
        "# Crear el DataFrame de Spark a partir de los nuevos datos\n",
        "nuevos_datos_spark = spark.createDataFrame([nuevo_registro], schema=schema)\n",
        "\n",
        "# Verificar las primeras filas del DataFrame de Spark con los nuevos datos\n",
        "nuevos_datos_spark.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg6mqj0YRCOF",
        "outputId": "8566d8b5-9a37-4a9e-805e-b9a079601e3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "2024-01-16 03:15:26.388724: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 03:15:26.388790: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 03:15:26.390736: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 03:15:28.090898: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting es-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.6.0/es_core_news_sm-3.6.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.3)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXyrlqwfQwXq",
        "outputId": "6fb277de-bac4-4da6-9393-41eadecd1512"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "from pyspark.ml import Pipeline\n",
        "import pyspark.sql.functions as F\n",
        "import nltk\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Cargar modelo de procesamiento de lenguaje natural en español\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "# Obtener las stopwords en español\n",
        "spanish_stop_words = set(nltk.corpus.stopwords.words('spanish'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "aC9gejNbbYjn",
        "outputId": "662947a0-9366-4fe5-b651-c87dc34a4cf2"
      },
      "outputs": [
        {
          "ename": "PythonException",
          "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-41-39030096985e>\", line 9, in lemmatize_text\n  File \"/usr/local/lib/python3.10/dist-packages/spacy/language.py\", line 1030, in __call__\n    doc = self._ensure_doc(text)\n  File \"/usr/local/lib/python3.10/dist-packages/spacy/language.py\", line 1124, in _ensure_doc\n    raise ValueError(Errors.E1041.format(type=type(doc_like)))\nValueError: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'list'>\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-493349b1bf7d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Verificar las primeras filas del DataFrame de Spark con los nuevos datos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnuevos_datos_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Imprimir el esquema del DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnuevos_datos_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-41-39030096985e>\", line 9, in lemmatize_text\n  File \"/usr/local/lib/python3.10/dist-packages/spacy/language.py\", line 1030, in __call__\n    doc = self._ensure_doc(text)\n  File \"/usr/local/lib/python3.10/dist-packages/spacy/language.py\", line 1124, in _ensure_doc\n    raise ValueError(Errors.E1041.format(type=type(doc_like)))\nValueError: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'list'>\n"
          ]
        }
      ],
      "source": [
        "# Verificar las primeras filas del DataFrame de Spark con los nuevos datos\n",
        "nuevos_datos_spark.show()\n",
        "\n",
        "# Imprimir el esquema del DataFrame\n",
        "nuevos_datos_spark.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "A0mgoAgqRsuB",
        "outputId": "89127139-1522-4b87-cef0-9957c0ac2e05"
      },
      "outputs": [
        {
          "ename": "IllegalArgumentException",
          "evalue": "features does not exist. Available: sellerName, product_name, shop_sku, primary_category, N1, width_in_cm, length_in_cm, height_in_cm, weight_in_kg, volumen_cm3, size, product_name_tokens, primary_category_tokens, product_name_cleaned, primary_category_cleaned",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-39030096985e>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# TF-IDF en 'product_name' y 'primary_category'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Asumiendo que 'loaded_model' y 'model_category' contienen los modelos que necesitas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mnuevos_datos_spark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnuevos_datos_spark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mnuevos_datos_spark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_category\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnuevos_datos_spark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: features does not exist. Available: sellerName, product_name, shop_sku, primary_category, N1, width_in_cm, length_in_cm, height_in_cm, weight_in_kg, volumen_cm3, size, product_name_tokens, primary_category_tokens, product_name_cleaned, primary_category_cleaned"
          ]
        }
      ],
      "source": [
        "# Definir funciones de tokenización y limpieza\n",
        "def tokenize_and_clean(text):\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in spanish_stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Definir función de lematización\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    return ' '.join([token.lemma_ for token in doc])\n",
        "\n",
        "# Crear UDFs de Spark\n",
        "tokenize_and_clean_spark_udf = udf(tokenize_and_clean_udf, ArrayType(StringType()))\n",
        "lemmatize_text_spark_udf = udf(lemmatize_text_udf, StringType())\n",
        "\n",
        "# Aplicar las transformaciones de texto en un solo paso\n",
        "df_spark = df_spark.withColumn('product_name_tokens', tokenize_and_clean_spark_udf('product_name'))\n",
        "df_spark = df_spark.withColumn('primary_category_tokens', tokenize_and_clean_spark_udf('primary_category'))\n",
        "df_spark = df_spark.withColumn('product_name_cleaned', lemmatize_text_spark_udf(F.concat_ws(' ', 'product_name_tokens')))\n",
        "df_spark = df_spark.withColumn('primary_category_cleaned', lemmatize_text_spark_udf(F.concat_ws(' ', 'primary_category_tokens')))\n",
        "\n",
        "# TF-IDF en 'product_name' y 'primary_category'\n",
        "# Asumiendo que 'loaded_model' y 'model_category' contienen los modelos que necesitas\n",
        "nuevos_datos_spark = loaded_model.transform(nuevos_datos_spark)\n",
        "nuevos_datos_spark = model_category.transform(nuevos_datos_spark)\n",
        "\n",
        "# Reducción de palabras en las columnas lematizadas\n",
        "nuevos_datos_spark = nuevos_datos_spark.withColumn('product_name_cleaned', F.expr(\"substring_index(product_name_cleaned, ' ', 2)\"))\n",
        "nuevos_datos_spark = nuevos_datos_spark.withColumn('primary_category_cleaned', F.expr(\"substring_index(primary_category_cleaned, ' ', 2)\"))\n",
        "\n",
        "# Asumiendo que las columnas de características son 'name_tfidf', 'category_tfidf', 'product_name_cleaned', y 'primary_category_cleaned'\n",
        "feature_columns = ['name_tfidf', 'category_tfidf', 'product_name_cleaned', 'primary_category_cleaned']\n",
        "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
        "\n",
        "# Ajustar columnas de características\n",
        "nuevos_datos_spark = vector_assembler.transform(nuevos_datos_spark)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones_nuevos_datos = loaded_model.transform(nuevos_datos_spark)\n",
        "\n",
        "# Visualizar las predicciones\n",
        "predicciones_nuevos_datos.select('prediction').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDTDpRgmS_cI"
      },
      "source": [
        "# de aca abajo se mantiene"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "E6f5dNUhIfzG",
        "outputId": "37f00f37-4334-4c2f-c745-def4f30afd38"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tokenize_and_clean_spark_udf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a204b17f14db>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Tokenización y limpieza en 'product_name' y 'primary_category'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnuevos_datos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnuevos_datos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'product_name_tokens'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_and_clean_spark_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'product_name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mnuevos_datos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnuevos_datos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'primary_category_tokens'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_and_clean_spark_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'primary_category'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenize_and_clean_spark_udf' is not defined"
          ]
        }
      ],
      "source": [
        "# Crear un DataFrame de Spark con el nuevo registro\n",
        "nuevos_datos = spark.createDataFrame([nuevo_registro], schema=schema)\n",
        "\n",
        "# Tokenización y limpieza en 'product_name' y 'primary_category'\n",
        "nuevos_datos = nuevos_datos.withColumn('product_name_tokens', tokenize_and_clean_spark_udf('product_name'))\n",
        "nuevos_datos = nuevos_datos.withColumn('primary_category_tokens', tokenize_and_clean_spark_udf('primary_category'))\n",
        "\n",
        "# Lematización en 'product_name' y 'primary_category'\n",
        "nuevos_datos = nuevos_datos.withColumn('product_name_cleaned', lemmatize_text_spark_udf('product_name_tokens'))\n",
        "nuevos_datos = nuevos_datos.withColumn('primary_category_cleaned', lemmatize_text_spark_udf('primary_category_tokens'))\n",
        "\n",
        "# TF-IDF en 'product_name' y 'primary_category'\n",
        "nuevos_datos = model_name.transform(nuevos_datos)\n",
        "nuevos_datos = model_category.transform(nuevos_datos)\n",
        "\n",
        "# Reducción de palabras en las columnas lematizadas\n",
        "nuevos_datos = nuevos_datos.withColumn('product_name_cleaned', F.expr(\"substring_index(product_name_cleaned, ' ', 2)\"))\n",
        "nuevos_datos = nuevos_datos.withColumn('primary_category_cleaned', F.expr(\"substring_index(primary_category_cleaned, ' ', 2)\"))\n",
        "\n",
        "# Ajustar columnas de características\n",
        "nuevos_datos_assembled = vector_assembler.transform(nuevos_datos).select('features')\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones_nuevos_datos = model.transform(nuevos_datos_assembled)\n",
        "\n",
        "# Visualizar las predicciones\n",
        "predicciones_nuevos_datos.select('prediction').show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY6kwsIByOX3"
      },
      "outputs": [],
      "source": [
        "# Crear un DataFrame de Spark con el nuevo registro\n",
        "nuevos_datos = spark.createDataFrame([nuevo_registro], schema=schema)\n",
        "\n",
        "# Tokenización y limpieza en 'product_name' y 'primary_category'\n",
        "nuevos_datos = nuevos_datos.withColumn('product_name_tokens', tokenize_and_clean_spark_udf('product_name'))\n",
        "nuevos_datos = nuevos_datos.withColumn('primary_category_tokens', tokenize_and_clean_spark_udf('primary_category'))\n",
        "\n",
        "# Lematización en 'product_name' y 'primary_category'\n",
        "nuevos_datos = nuevos_datos.withColumn('product_name_cleaned', lemmatize_text_spark_udf('product_name_tokens'))\n",
        "nuevos_datos = nuevos_datos.withColumn('primary_category_cleaned', lemmatize_text_spark_udf('primary_category_tokens'))\n",
        "\n",
        "# TF-IDF en 'product_name' y 'primary_category'\n",
        "nuevos_datos = model_name.transform(nuevos_datos)\n",
        "nuevos_datos = model_category.transform(nuevos_datos)\n",
        "\n",
        "# Reducción de palabras en las columnas lematizadas\n",
        "nuevos_datos = nuevos_datos.withColumn('product_name_cleaned', F.expr(\"substring_index(product_name_cleaned, ' ', 2)\"))\n",
        "nuevos_datos = nuevos_datos.withColumn('primary_category_cleaned', F.expr(\"substring_index(primary_category_cleaned, ' ', 2)\"))\n",
        "\n",
        "# Ajustar columnas de características\n",
        "nuevos_datos_assembled = vector_assembler.transform(nuevos_datos).select('features')\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones_nuevos_datos = model.transform(nuevos_datos_assembled)\n",
        "\n",
        "# Crear un DataFrame de mapeo entre prediction y Size\n",
        "size_label_mapping = [\n",
        "    (0.0, 'XS2'),\n",
        "    (1.0, 'XS3'),\n",
        "    (2.0, 'XS'),\n",
        "    (3.0, 'M'),\n",
        "    (4.0, 'S'),\n",
        "    (5.0, 'L'),\n",
        "    (6.0, 'EO'),\n",
        "    (7.0, 'XL'),\n",
        "    (8.0, 'LO'),\n",
        "    (9.0, 'O'),\n",
        "    (10.0, 'XXL')\n",
        "]\n",
        "\n",
        "size_label_mapping_df = spark.createDataFrame(size_label_mapping, ['label', 'Size'])\n",
        "\n",
        "# Realizar join con el DataFrame de mapping\n",
        "predicciones_nuevos_datos_with_size = predicciones_nuevos_datos.join(\n",
        "    size_label_mapping_df,\n",
        "    predicciones_nuevos_datos.prediction == size_label_mapping_df.label\n",
        ")\n",
        "\n",
        "# Seleccionar solo las columnas necesarias\n",
        "result_df = predicciones_nuevos_datos_with_size.select('prediction', 'Size')\n",
        "\n",
        "# Visualizar las predicciones con el tamaño correspondiente\n",
        "result_df.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}